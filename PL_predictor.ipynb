{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.stats\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt \n",
    "from keras import optimizers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the excel file\n",
    "loc = (\"Premier_League_Data.xlsx\")\n",
    "wb = xlrd.open_workbook(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parameters = 22\n",
    "num_teams = 20\n",
    "num_stats_sheets = 7\n",
    "num_total_sheets = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sheets\n",
    "\n",
    "stats_sheets = []\n",
    "for i in range(0, num_stats_sheets):\n",
    "    stats_sheets.append(wb.sheet_by_index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_by_season = []\n",
    "teams_per_season = []\n",
    "promoted_per_season = []\n",
    "relegated_per_season = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_this_year = {}\n",
    "teams_this_year = []\n",
    "promoted_this_year = []\n",
    "relegated_this_year = []\n",
    "for m in range(0, num_stats_sheets):\n",
    "    for i in range(1, num_teams + 1):\n",
    "        team = stats_sheets[m].cell_value(i, 0)\n",
    "        teams_this_year.append(team)\n",
    "        stats_this_year[team] = []\n",
    "        for j in range(1, num_parameters + 1):\n",
    "            stats_this_year[team].append(stats_sheets[m].cell_value(i, j))\n",
    "            \n",
    "    relegated_this_year.append(teams_this_year[num_teams - 3])\n",
    "    relegated_this_year.append(teams_this_year[num_teams - 2])\n",
    "    relegated_this_year.append(teams_this_year[num_teams - 1])\n",
    "\n",
    "    promoted_this_year.append(stats_sheets[m].cell_value(num_teams + 3, 0))\n",
    "    promoted_this_year.append(stats_sheets[m].cell_value(num_teams + 4, 0))\n",
    "    promoted_this_year.append(stats_sheets[m].cell_value(num_teams + 5, 0))\n",
    "\n",
    "    stats_by_season.append(stats_this_year)\n",
    "    teams_per_season.append(teams_this_year)\n",
    "    promoted_per_season.append(promoted_this_year)\n",
    "    relegated_per_season.append(relegated_this_year)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get promoted teams from 2010/11 (no other info on this sheet, so has to be hard-coded in)\n",
    "\n",
    "stats_1011_sheet = wb.sheet_by_index(13)\n",
    "title = stats_1011_sheet.cell_value(0, 0)\n",
    "promoted_1011 = []\n",
    "for i in range(1, 4):\n",
    "    team = stats_1011_sheet.cell_value(i, 0)\n",
    "    promoted_1011.append(team.strip())\n",
    "promoted_per_season.append(promoted_1011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sheets = []\n",
    "for i in range(num_stats_sheets, num_total_sheets):\n",
    "    score_sheets.append(wb.sheet_by_index(i))\n",
    "\n",
    "# scores_1718_sheet = wb.sheet_by_index(7)\n",
    "# scores_1617_sheet = wb.sheet_by_index(8)\n",
    "# scores_1516_sheet = wb.sheet_by_index(9)\n",
    "# scores_1415_sheet = wb.sheet_by_index(10)\n",
    "# scores_1314_sheet = wb.sheet_by_index(11)\n",
    "# scores_1213_sheet = wb.sheet_by_index(12)\n",
    "# score_sheets = []\n",
    "# score_sheets.append(scores_1718_sheet)\n",
    "# score_sheets.append(scores_1617_sheet)\n",
    "# score_sheets.append(scores_1516_sheet)\n",
    "# score_sheets.append(scores_1415_sheet)\n",
    "# score_sheets.append(scores_1314_sheet)\n",
    "# score_sheets.append(scores_1213_sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "1710\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_stats = []\n",
    "all_outcomes = []\n",
    "stats_pairs_by_season = []\n",
    "outcomes_by_season = []\n",
    "\n",
    "for i in range (0 , 6):\n",
    "    results = {}\n",
    "    for j in range (1, num_teams * (num_teams - 1) + 1):\n",
    "        home_team = score_sheets[i].cell_value(j, 1).strip()\n",
    "        away_team = score_sheets[i].cell_value(j, 2).strip()\n",
    "        result = score_sheets[i].cell_value(j, 5)\n",
    "        if result == \"W\":\n",
    "            results[(home_team, away_team)] = 1\n",
    "        elif result == \"L\":\n",
    "            results[(home_team, away_team)] = -1\n",
    "        else:\n",
    "            results[(home_team, away_team)] = 0\n",
    "    all_results.append(results)\n",
    "    stats = []\n",
    "    outcomes = []\n",
    "    # print(i, len(results))  \n",
    "    # print(i, len(all_results))\n",
    "    counter = 0\n",
    "    for key in results.keys():\n",
    "        counter = counter + 1\n",
    "        # each home-away pair for that season\n",
    "        home_team = key[0].strip()\n",
    "        away_team = key[1].strip()\n",
    "        prev_season_stats = stats_by_season[i + 1]\n",
    "        if home_team in prev_season_stats.keys():\n",
    "            home_stats = prev_season_stats[home_team]\n",
    "        else:\n",
    "            promoted_index = promoted_per_season[i+1].index(home_team)\n",
    "            promoted_team = promoted_per_season[i+2][promoted_index] # out of bounds error?\n",
    "            home_stats = prev_season_stats[promoted_team]\n",
    "        if away_team in prev_season_stats.keys():\n",
    "            away_stats = prev_season_stats[away_team]\n",
    "        else:\n",
    "            promoted_index = promoted_per_season[i + 1].index(away_team)\n",
    "            promoted_team = promoted_per_season[i + 2][promoted_index] # out of bounds error?\n",
    "            away_stats = prev_season_stats[promoted_team]\n",
    "        instance = []\n",
    "        for k in range (0, len(home_stats)):\n",
    "            #instance.append(home_stats[k])\n",
    "            #instance.append(away_stats[k])\n",
    "            if (away_stats[k] == 0):\n",
    "                ratio = (home_stats[k] + 1) / (away_stats[k] + 1)\n",
    "            else:\n",
    "                ratio = home_stats[k] / away_stats[k]\n",
    "            ratio = home_stats[k] - away_stats[k]\n",
    "            instance.append(ratio)\n",
    "        outcome = results[(home_team, away_team)]\n",
    "        # print(home_team, away_team, ratio, outcome)\n",
    "        if (outcome !=0):\n",
    "            stats.append(instance)\n",
    "            all_stats.append(instance)\n",
    "            outcomes.append(outcome)\n",
    "            all_outcomes.append(outcome)\n",
    "    stats_pairs_by_season.append(stats)\n",
    "    outcomes_by_season.append(outcomes)\n",
    "print(len(all_stats))\n",
    "# print(all_stats)\n",
    "# print(all_outcomes)\n",
    "print(len(all_outcomes))\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Feature matrix being passed into neural network has 1457 vectors and 22 parameters. \n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_stats, all_outcomes, test_size = 0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)\n",
    "\n",
    "# convert data into numpy arrays\n",
    "# x_train = np.array(x_train)\n",
    "# y_train = np.array(y_train)\n",
    "# x_train = np.array(x_train)\n",
    "# y_train = np.array(y_train)\n",
    "# x_val = np.array(x_val)\n",
    "# y_val = np.array(y_val)\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "num_records = 1457\n",
    "# num_records = x_train.shape[0]\n",
    "num_features = 22\n",
    "# num_features = x_train.shape[1]\n",
    "\n",
    "\n",
    "print(\"Training Feature matrix being passed into neural network has \" + str(num_records) + \n",
    "      \" vectors and \" + str(num_features) + \" parameters. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_maxent_classifier(X, y, testFeatureVectors): \n",
    "    mod = LogisticRegression(fit_intercept=True, multi_class = 'multinomial', solver = 'newton-cg')\n",
    "    mod.fit(X, y)\n",
    "    pred = mod.predict(testFeatureVectors)\n",
    "    print(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1  1  1  1 -1 -1  1  1  1 -1  1  1  1 -1  1 -1  1  1 -1  1 -1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1 -1 -1  1  1  1  1\n",
      "  1 -1  1 -1  1 -1  1 -1  1  1  1  1 -1 -1  1 -1 -1  1  1  1  1  1  1 -1\n",
      " -1  1 -1 -1  1  1  1  1 -1 -1  1  1  1  1 -1 -1 -1 -1  1 -1  1 -1  1 -1\n",
      "  1  1 -1  1 -1 -1 -1 -1  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1\n",
      " -1  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1  1  1  1\n",
      "  1 -1  1  1 -1  1  1  1 -1  1  1 -1  1  1  1  1  1 -1  1  1  1 -1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1 -1 -1 -1 -1 -1  1  1  1\n",
      "  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1 -1 -1  1  1 -1  1  1  1 -1 -1  1  1\n",
      "  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1  1  1  1  1  1  1  1  1  1 -1 -1  1\n",
      "  1  1 -1  1  1  1 -1  1  1 -1]\n",
      "195\n",
      "274\n",
      "0.7116788321167883\n"
     ]
    }
   ],
   "source": [
    "pred = fit_maxent_classifier(x_train, y_train, x_val)\n",
    "\n",
    "correct = 0\n",
    "total = len(y_val)\n",
    "for i in range(0, len(y_val)):\n",
    "    if (y_val[i]==pred[i]):\n",
    "        correct+=1\n",
    "    else:\n",
    "        if (y_val[i]==0):\n",
    "            total = total - 1\n",
    "            print(y_val[i])\n",
    "        \n",
    "print(correct)\n",
    "print(total)\n",
    "print(correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_nn_classifier(learning_rate):\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add the layers\n",
    "    model.add(Dense(1000, input_dim = num_features, kernel_regularizer = regularizers.l2(0.1), activation = 'sigmoid')) # hidden layer\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(750, kernel_regularizer = regularizers.l2(0.01), activation = 'sigmoid')) # second hidden layer\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(300, activation = 'sigmoid')) # second hidden layer\n",
    "    model.add(Dense(100, activation = 'sigmoid')) # second hidden layer\n",
    "    #model.add(Dense(25, activation = 'sigmoid')) # second hidden layer\n",
    "\n",
    "    model.add(Dense(num_classes, activation = 'softmax')) # output layer\n",
    "        \n",
    "    # optimizers\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate) # model doesn't learn \n",
    "    adam = keras.optimizers.Adam(lr = learning_rate)   \n",
    "    rmsprop = keras.optimizers.RMSprop(lr = learning_rate) \n",
    "    adagrad = keras.optimizers.Adagrad(lr = learning_rate) # bad\n",
    "    adadelta = keras.optimizers.Adadelta(lr = learning_rate) # bad\n",
    "    adamax = keras.optimizers.Adamax(lr = learning_rate) # not terrible, like 0.55\n",
    "    nadam = keras.optimizers.Nadam(lr = learning_rate) # pretty good, 0.58\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss = keras.losses.categorical_crossentropy, optimizer = adam, metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_151 (Dense)            (None, 1000)              23000     \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 123,302\n",
      "Trainable params: 123,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(274, 22)\n",
      "(274, 2)\n",
      "(1094, 22)\n",
      "(1094, 2)\n",
      "Train on 1094 samples, validate on 274 samples\n",
      "Epoch 1/500\n",
      " - 2s - loss: 4.3634 - acc: 1.0000 - val_loss: 3.8397 - val_acc: 1.0000\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.7202 - acc: 1.0000 - val_loss: 3.3868 - val_acc: 1.0000\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2802 - acc: 1.0000 - val_loss: 2.9797 - val_acc: 1.0000\n",
      "Epoch 4/500\n",
      " - 0s - loss: 2.8832 - acc: 1.0000 - val_loss: 2.6116 - val_acc: 1.0000\n",
      "Epoch 5/500\n",
      " - 0s - loss: 2.5246 - acc: 1.0000 - val_loss: 2.2799 - val_acc: 1.0000\n",
      "Epoch 6/500\n",
      " - 0s - loss: 2.2019 - acc: 1.0000 - val_loss: 1.9828 - val_acc: 1.0000\n",
      "Epoch 7/500\n",
      " - 0s - loss: 1.9131 - acc: 1.0000 - val_loss: 1.7179 - val_acc: 1.0000\n",
      "Epoch 8/500\n",
      " - 0s - loss: 1.6560 - acc: 1.0000 - val_loss: 1.4828 - val_acc: 1.0000\n",
      "Epoch 9/500\n",
      " - 0s - loss: 1.4281 - acc: 1.0000 - val_loss: 1.2753 - val_acc: 1.0000\n",
      "Epoch 10/500\n",
      " - 0s - loss: 1.2271 - acc: 1.0000 - val_loss: 1.0928 - val_acc: 1.0000\n",
      "Epoch 11/500\n",
      " - 0s - loss: 1.0506 - acc: 1.0000 - val_loss: 0.9325 - val_acc: 1.0000\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.8958 - acc: 1.0000 - val_loss: 0.7934 - val_acc: 1.0000\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.7614 - acc: 1.0000 - val_loss: 0.6722 - val_acc: 1.0000\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.6446 - acc: 1.0000 - val_loss: 0.5679 - val_acc: 1.0000\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.5440 - acc: 1.0000 - val_loss: 0.4777 - val_acc: 1.0000\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.4573 - acc: 1.0000 - val_loss: 0.4007 - val_acc: 1.0000\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.3832 - acc: 1.0000 - val_loss: 0.3348 - val_acc: 1.0000\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.3199 - acc: 1.0000 - val_loss: 0.2787 - val_acc: 1.0000\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.2661 - acc: 1.0000 - val_loss: 0.2312 - val_acc: 1.0000\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.2205 - acc: 1.0000 - val_loss: 0.1910 - val_acc: 1.0000\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.1820 - acc: 1.0000 - val_loss: 0.1572 - val_acc: 1.0000\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.1497 - acc: 1.0000 - val_loss: 0.1289 - val_acc: 1.0000\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.1226 - acc: 1.0000 - val_loss: 0.1052 - val_acc: 1.0000\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.1000 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 1.0000\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.0812 - acc: 1.0000 - val_loss: 0.0693 - val_acc: 1.0000\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.0657 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.0529 - acc: 1.0000 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.0424 - acc: 1.0000 - val_loss: 0.0358 - val_acc: 1.0000\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.0338 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 1.0000\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.0268 - acc: 1.0000 - val_loss: 0.0225 - val_acc: 1.0000\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.0212 - acc: 1.0000 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 1.0000\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 1.0000\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 9.5412e-04 - val_acc: 1.0000\n",
      "Epoch 43/500\n",
      " - 0s - loss: 8.9028e-04 - acc: 1.0000 - val_loss: 7.2191e-04 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      " - 0s - loss: 6.7401e-04 - acc: 1.0000 - val_loss: 5.4832e-04 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      " - 0s - loss: 5.1286e-04 - acc: 1.0000 - val_loss: 4.1999e-04 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      " - 0s - loss: 3.9369e-04 - acc: 1.0000 - val_loss: 3.2544e-04 - val_acc: 1.0000\n",
      "Epoch 47/500\n",
      " - 0s - loss: 3.0626e-04 - acc: 1.0000 - val_loss: 2.5648e-04 - val_acc: 1.0000\n",
      "Epoch 48/500\n",
      " - 0s - loss: 2.4267e-04 - acc: 1.0000 - val_loss: 2.0684e-04 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      " - 0s - loss: 1.9669e-04 - acc: 1.0000 - val_loss: 1.7089e-04 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      " - 0s - loss: 1.6380e-04 - acc: 1.0000 - val_loss: 1.4556e-04 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      " - 0s - loss: 1.4041e-04 - acc: 1.0000 - val_loss: 1.2772e-04 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      " - 0s - loss: 1.2390e-04 - acc: 1.0000 - val_loss: 1.1482e-04 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      " - 0s - loss: 1.1237e-04 - acc: 1.0000 - val_loss: 1.0617e-04 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      " - 0s - loss: 1.0433e-04 - acc: 1.0000 - val_loss: 1.0014e-04 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      " - 0s - loss: 9.8779e-05 - acc: 1.0000 - val_loss: 9.5535e-05 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      " - 0s - loss: 9.4835e-05 - acc: 1.0000 - val_loss: 9.3083e-05 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      " - 0s - loss: 9.2169e-05 - acc: 1.0000 - val_loss: 9.0792e-05 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      " - 0s - loss: 9.0158e-05 - acc: 1.0000 - val_loss: 8.8964e-05 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      " - 0s - loss: 8.8816e-05 - acc: 1.0000 - val_loss: 8.8063e-05 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      " - 0s - loss: 8.7701e-05 - acc: 1.0000 - val_loss: 8.7119e-05 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      " - 0s - loss: 8.6852e-05 - acc: 1.0000 - val_loss: 8.6500e-05 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      " - 0s - loss: 8.6192e-05 - acc: 1.0000 - val_loss: 8.5855e-05 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      " - 0s - loss: 8.5538e-05 - acc: 1.0000 - val_loss: 8.5000e-05 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      " - 0s - loss: 8.5026e-05 - acc: 1.0000 - val_loss: 8.4906e-05 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      " - 0s - loss: 8.4525e-05 - acc: 1.0000 - val_loss: 8.4283e-05 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      " - 0s - loss: 8.3998e-05 - acc: 1.0000 - val_loss: 8.3583e-05 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      " - 0s - loss: 8.3512e-05 - acc: 1.0000 - val_loss: 8.3472e-05 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      " - 0s - loss: 8.3080e-05 - acc: 1.0000 - val_loss: 8.2934e-05 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      " - 0s - loss: 8.2604e-05 - acc: 1.0000 - val_loss: 8.2344e-05 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      " - 0s - loss: 8.2173e-05 - acc: 1.0000 - val_loss: 8.1969e-05 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      " - 0s - loss: 8.1723e-05 - acc: 1.0000 - val_loss: 8.1776e-05 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      " - 0s - loss: 8.1344e-05 - acc: 1.0000 - val_loss: 8.1003e-05 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      " - 0s - loss: 8.0951e-05 - acc: 1.0000 - val_loss: 8.0673e-05 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      " - 0s - loss: 8.0488e-05 - acc: 1.0000 - val_loss: 8.0616e-05 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      " - 0s - loss: 8.0110e-05 - acc: 1.0000 - val_loss: 7.9714e-05 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      " - 0s - loss: 7.9746e-05 - acc: 1.0000 - val_loss: 7.9481e-05 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      " - 0s - loss: 7.9327e-05 - acc: 1.0000 - val_loss: 7.9251e-05 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      " - 0s - loss: 7.8956e-05 - acc: 1.0000 - val_loss: 7.8613e-05 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      " - 0s - loss: 7.8590e-05 - acc: 1.0000 - val_loss: 7.8417e-05 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      " - 0s - loss: 7.8207e-05 - acc: 1.0000 - val_loss: 7.8030e-05 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      " - 0s - loss: 7.7847e-05 - acc: 1.0000 - val_loss: 7.7736e-05 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      " - 0s - loss: 7.7497e-05 - acc: 1.0000 - val_loss: 7.7321e-05 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      " - 0s - loss: 7.7141e-05 - acc: 1.0000 - val_loss: 7.7059e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      " - 0s - loss: 7.6821e-05 - acc: 1.0000 - val_loss: 7.6573e-05 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      " - 0s - loss: 7.6471e-05 - acc: 1.0000 - val_loss: 7.6186e-05 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      " - 0s - loss: 7.6132e-05 - acc: 1.0000 - val_loss: 7.6160e-05 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      " - 0s - loss: 7.5838e-05 - acc: 1.0000 - val_loss: 7.5616e-05 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      " - 0s - loss: 7.5503e-05 - acc: 1.0000 - val_loss: 7.5202e-05 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      " - 0s - loss: 7.5162e-05 - acc: 1.0000 - val_loss: 7.5275e-05 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      " - 0s - loss: 7.4873e-05 - acc: 1.0000 - val_loss: 7.4595e-05 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      " - 0s - loss: 7.4564e-05 - acc: 1.0000 - val_loss: 7.4216e-05 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      " - 0s - loss: 7.4286e-05 - acc: 1.0000 - val_loss: 7.4035e-05 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      " - 0s - loss: 7.3967e-05 - acc: 1.0000 - val_loss: 7.3677e-05 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      " - 0s - loss: 7.3643e-05 - acc: 1.0000 - val_loss: 7.3647e-05 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      " - 0s - loss: 7.3308e-05 - acc: 1.0000 - val_loss: 7.2968e-05 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      " - 0s - loss: 7.3071e-05 - acc: 1.0000 - val_loss: 7.2889e-05 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      " - 0s - loss: 7.2746e-05 - acc: 1.0000 - val_loss: 7.2806e-05 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      " - 0s - loss: 7.2503e-05 - acc: 1.0000 - val_loss: 7.2141e-05 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      " - 0s - loss: 7.2174e-05 - acc: 1.0000 - val_loss: 7.2138e-05 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      " - 0s - loss: 7.1865e-05 - acc: 1.0000 - val_loss: 7.1634e-05 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      " - 0s - loss: 7.1588e-05 - acc: 1.0000 - val_loss: 7.1505e-05 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      " - 0s - loss: 7.1295e-05 - acc: 1.0000 - val_loss: 7.1249e-05 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      " - 0s - loss: 7.1033e-05 - acc: 1.0000 - val_loss: 7.0858e-05 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      " - 0s - loss: 7.0729e-05 - acc: 1.0000 - val_loss: 7.0708e-05 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      " - 0s - loss: 7.0463e-05 - acc: 1.0000 - val_loss: 7.0284e-05 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      " - 0s - loss: 7.0220e-05 - acc: 1.0000 - val_loss: 7.0094e-05 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      " - 0s - loss: 6.9919e-05 - acc: 1.0000 - val_loss: 7.0118e-05 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      " - 0s - loss: 6.9705e-05 - acc: 1.0000 - val_loss: 6.9479e-05 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      " - 0s - loss: 6.9406e-05 - acc: 1.0000 - val_loss: 6.9359e-05 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      " - 0s - loss: 6.9169e-05 - acc: 1.0000 - val_loss: 6.9128e-05 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      " - 0s - loss: 6.9009e-05 - acc: 1.0000 - val_loss: 6.8776e-05 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      " - 0s - loss: 6.8661e-05 - acc: 1.0000 - val_loss: 6.8885e-05 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      " - 0s - loss: 6.8391e-05 - acc: 1.0000 - val_loss: 6.8038e-05 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      " - 0s - loss: 6.8184e-05 - acc: 1.0000 - val_loss: 6.8088e-05 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      " - 0s - loss: 6.7878e-05 - acc: 1.0000 - val_loss: 6.7800e-05 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      " - 0s - loss: 6.7615e-05 - acc: 1.0000 - val_loss: 6.7365e-05 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      " - 0s - loss: 6.7371e-05 - acc: 1.0000 - val_loss: 6.7412e-05 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      " - 0s - loss: 6.7122e-05 - acc: 1.0000 - val_loss: 6.6917e-05 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      " - 0s - loss: 6.6882e-05 - acc: 1.0000 - val_loss: 6.6694e-05 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      " - 0s - loss: 6.6622e-05 - acc: 1.0000 - val_loss: 6.6481e-05 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      " - 0s - loss: 6.6375e-05 - acc: 1.0000 - val_loss: 6.6415e-05 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      " - 0s - loss: 6.6154e-05 - acc: 1.0000 - val_loss: 6.5958e-05 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      " - 0s - loss: 6.5908e-05 - acc: 1.0000 - val_loss: 6.5813e-05 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      " - 0s - loss: 6.5652e-05 - acc: 1.0000 - val_loss: 6.5498e-05 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      " - 0s - loss: 6.5415e-05 - acc: 1.0000 - val_loss: 6.5288e-05 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      " - 0s - loss: 6.5199e-05 - acc: 1.0000 - val_loss: 6.5109e-05 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      " - 0s - loss: 6.4953e-05 - acc: 1.0000 - val_loss: 6.4739e-05 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      " - 0s - loss: 6.4718e-05 - acc: 1.0000 - val_loss: 6.4762e-05 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      " - 0s - loss: 6.4499e-05 - acc: 1.0000 - val_loss: 6.4384e-05 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      " - 0s - loss: 6.4249e-05 - acc: 1.0000 - val_loss: 6.4069e-05 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      " - 0s - loss: 6.4046e-05 - acc: 1.0000 - val_loss: 6.3878e-05 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      " - 0s - loss: 6.3788e-05 - acc: 1.0000 - val_loss: 6.3804e-05 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      " - 0s - loss: 6.3575e-05 - acc: 1.0000 - val_loss: 6.3436e-05 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      " - 0s - loss: 6.3344e-05 - acc: 1.0000 - val_loss: 6.3153e-05 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      " - 0s - loss: 6.3163e-05 - acc: 1.0000 - val_loss: 6.3004e-05 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      " - 0s - loss: 6.2941e-05 - acc: 1.0000 - val_loss: 6.2754e-05 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      " - 0s - loss: 6.2797e-05 - acc: 1.0000 - val_loss: 6.2488e-05 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      " - 0s - loss: 6.2501e-05 - acc: 1.0000 - val_loss: 6.2625e-05 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      " - 0s - loss: 6.2256e-05 - acc: 1.0000 - val_loss: 6.1942e-05 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      " - 0s - loss: 6.2076e-05 - acc: 1.0000 - val_loss: 6.2007e-05 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      " - 0s - loss: 6.1840e-05 - acc: 1.0000 - val_loss: 6.1629e-05 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      " - 0s - loss: 6.1648e-05 - acc: 1.0000 - val_loss: 6.1355e-05 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      " - 0s - loss: 6.1429e-05 - acc: 1.0000 - val_loss: 6.1556e-05 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      " - 0s - loss: 6.1200e-05 - acc: 1.0000 - val_loss: 6.0866e-05 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      " - 0s - loss: 6.0968e-05 - acc: 1.0000 - val_loss: 6.1014e-05 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      " - 0s - loss: 6.0750e-05 - acc: 1.0000 - val_loss: 6.0579e-05 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      " - 0s - loss: 6.0508e-05 - acc: 1.0000 - val_loss: 6.0366e-05 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      " - 0s - loss: 6.0291e-05 - acc: 1.0000 - val_loss: 6.0378e-05 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      " - 0s - loss: 6.0093e-05 - acc: 1.0000 - val_loss: 5.9893e-05 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      " - 0s - loss: 5.9888e-05 - acc: 1.0000 - val_loss: 5.9691e-05 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      " - 0s - loss: 5.9707e-05 - acc: 1.0000 - val_loss: 5.9556e-05 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      " - 0s - loss: 5.9480e-05 - acc: 1.0000 - val_loss: 5.9218e-05 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      " - 0s - loss: 5.9299e-05 - acc: 1.0000 - val_loss: 5.9294e-05 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      " - 0s - loss: 5.9032e-05 - acc: 1.0000 - val_loss: 5.8772e-05 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      " - 0s - loss: 5.8874e-05 - acc: 1.0000 - val_loss: 5.8760e-05 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      " - 0s - loss: 5.8636e-05 - acc: 1.0000 - val_loss: 5.8494e-05 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      " - 0s - loss: 5.8417e-05 - acc: 1.0000 - val_loss: 5.8156e-05 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      " - 0s - loss: 5.8259e-05 - acc: 1.0000 - val_loss: 5.8235e-05 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      " - 0s - loss: 5.8064e-05 - acc: 1.0000 - val_loss: 5.8122e-05 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      " - 0s - loss: 5.7834e-05 - acc: 1.0000 - val_loss: 5.7603e-05 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      " - 0s - loss: 5.7638e-05 - acc: 1.0000 - val_loss: 5.7556e-05 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      " - 0s - loss: 5.7429e-05 - acc: 1.0000 - val_loss: 5.7218e-05 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      " - 0s - loss: 5.7255e-05 - acc: 1.0000 - val_loss: 5.7022e-05 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      " - 0s - loss: 5.7031e-05 - acc: 1.0000 - val_loss: 5.7144e-05 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      " - 0s - loss: 5.6854e-05 - acc: 1.0000 - val_loss: 5.6623e-05 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      " - 0s - loss: 5.6624e-05 - acc: 1.0000 - val_loss: 5.6620e-05 - val_acc: 1.0000\n",
      "Epoch 167/500\n",
      " - 0s - loss: 5.6452e-05 - acc: 1.0000 - val_loss: 5.6287e-05 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      " - 0s - loss: 5.6238e-05 - acc: 1.0000 - val_loss: 5.6016e-05 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      " - 0s - loss: 5.6046e-05 - acc: 1.0000 - val_loss: 5.6114e-05 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      " - 0s - loss: 5.5851e-05 - acc: 1.0000 - val_loss: 5.5642e-05 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      " - 0s - loss: 5.5664e-05 - acc: 1.0000 - val_loss: 5.5447e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      " - 0s - loss: 5.5471e-05 - acc: 1.0000 - val_loss: 5.5390e-05 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      " - 0s - loss: 5.5272e-05 - acc: 1.0000 - val_loss: 5.5100e-05 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      " - 0s - loss: 5.5092e-05 - acc: 1.0000 - val_loss: 5.5000e-05 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      " - 0s - loss: 5.4906e-05 - acc: 1.0000 - val_loss: 5.4689e-05 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      " - 0s - loss: 5.4694e-05 - acc: 1.0000 - val_loss: 5.4686e-05 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      " - 0s - loss: 5.4497e-05 - acc: 1.0000 - val_loss: 5.4321e-05 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      " - 0s - loss: 5.4344e-05 - acc: 1.0000 - val_loss: 5.4178e-05 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      " - 0s - loss: 5.4145e-05 - acc: 1.0000 - val_loss: 5.4015e-05 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      " - 0s - loss: 5.3940e-05 - acc: 1.0000 - val_loss: 5.3741e-05 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      " - 0s - loss: 5.3752e-05 - acc: 1.0000 - val_loss: 5.3788e-05 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      " - 0s - loss: 5.3584e-05 - acc: 1.0000 - val_loss: 5.3484e-05 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      " - 0s - loss: 5.3373e-05 - acc: 1.0000 - val_loss: 5.3316e-05 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      " - 0s - loss: 5.3192e-05 - acc: 1.0000 - val_loss: 5.3096e-05 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      " - 0s - loss: 5.3023e-05 - acc: 1.0000 - val_loss: 5.2900e-05 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      " - 0s - loss: 5.2856e-05 - acc: 1.0000 - val_loss: 5.2704e-05 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      " - 0s - loss: 5.2659e-05 - acc: 1.0000 - val_loss: 5.2763e-05 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      " - 0s - loss: 5.2521e-05 - acc: 1.0000 - val_loss: 5.2311e-05 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      " - 0s - loss: 5.2281e-05 - acc: 1.0000 - val_loss: 5.2325e-05 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      " - 0s - loss: 5.2117e-05 - acc: 1.0000 - val_loss: 5.1972e-05 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      " - 0s - loss: 5.1919e-05 - acc: 1.0000 - val_loss: 5.1918e-05 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      " - 0s - loss: 5.1754e-05 - acc: 1.0000 - val_loss: 5.1692e-05 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      " - 0s - loss: 5.1554e-05 - acc: 1.0000 - val_loss: 5.1367e-05 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      " - 0s - loss: 5.1401e-05 - acc: 1.0000 - val_loss: 5.1368e-05 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      " - 0s - loss: 5.1225e-05 - acc: 1.0000 - val_loss: 5.1173e-05 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      " - 0s - loss: 5.1048e-05 - acc: 1.0000 - val_loss: 5.0909e-05 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      " - 0s - loss: 5.0860e-05 - acc: 1.0000 - val_loss: 5.0897e-05 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      " - 0s - loss: 5.0693e-05 - acc: 1.0000 - val_loss: 5.0550e-05 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      " - 0s - loss: 5.0518e-05 - acc: 1.0000 - val_loss: 5.0449e-05 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      " - 0s - loss: 5.0344e-05 - acc: 1.0000 - val_loss: 5.0303e-05 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      " - 0s - loss: 5.0150e-05 - acc: 1.0000 - val_loss: 4.9958e-05 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      " - 0s - loss: 5.0038e-05 - acc: 1.0000 - val_loss: 4.9925e-05 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      " - 0s - loss: 4.9877e-05 - acc: 1.0000 - val_loss: 4.9807e-05 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      " - 0s - loss: 4.9644e-05 - acc: 1.0000 - val_loss: 4.9425e-05 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      " - 0s - loss: 4.9525e-05 - acc: 1.0000 - val_loss: 4.9462e-05 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      " - 0s - loss: 4.9314e-05 - acc: 1.0000 - val_loss: 4.9198e-05 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      " - 0s - loss: 4.9156e-05 - acc: 1.0000 - val_loss: 4.9055e-05 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      " - 0s - loss: 4.8967e-05 - acc: 1.0000 - val_loss: 4.8900e-05 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      " - 0s - loss: 4.8802e-05 - acc: 1.0000 - val_loss: 4.8719e-05 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      " - 0s - loss: 4.8638e-05 - acc: 1.0000 - val_loss: 4.8591e-05 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      " - 0s - loss: 4.8464e-05 - acc: 1.0000 - val_loss: 4.8315e-05 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      " - 0s - loss: 4.8321e-05 - acc: 1.0000 - val_loss: 4.8242e-05 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      " - 0s - loss: 4.8161e-05 - acc: 1.0000 - val_loss: 4.8117e-05 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      " - 0s - loss: 4.7980e-05 - acc: 1.0000 - val_loss: 4.7796e-05 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      " - 0s - loss: 4.7819e-05 - acc: 1.0000 - val_loss: 4.7814e-05 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      " - 0s - loss: 4.7659e-05 - acc: 1.0000 - val_loss: 4.7554e-05 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      " - 0s - loss: 4.7486e-05 - acc: 1.0000 - val_loss: 4.7348e-05 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      " - 0s - loss: 4.7320e-05 - acc: 1.0000 - val_loss: 4.7251e-05 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      " - 0s - loss: 4.7157e-05 - acc: 1.0000 - val_loss: 4.7035e-05 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      " - 0s - loss: 4.6999e-05 - acc: 1.0000 - val_loss: 4.6897e-05 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      " - 0s - loss: 4.6830e-05 - acc: 1.0000 - val_loss: 4.6837e-05 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      " - 0s - loss: 4.6678e-05 - acc: 1.0000 - val_loss: 4.6543e-05 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      " - 0s - loss: 4.6529e-05 - acc: 1.0000 - val_loss: 4.6430e-05 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      " - 0s - loss: 4.6357e-05 - acc: 1.0000 - val_loss: 4.6333e-05 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      " - 0s - loss: 4.6202e-05 - acc: 1.0000 - val_loss: 4.6061e-05 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      " - 0s - loss: 4.6059e-05 - acc: 1.0000 - val_loss: 4.5913e-05 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      " - 0s - loss: 4.5889e-05 - acc: 1.0000 - val_loss: 4.5876e-05 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      " - 0s - loss: 4.5738e-05 - acc: 1.0000 - val_loss: 4.5637e-05 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      " - 0s - loss: 4.5573e-05 - acc: 1.0000 - val_loss: 4.5541e-05 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      " - 0s - loss: 4.5420e-05 - acc: 1.0000 - val_loss: 4.5304e-05 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      " - 0s - loss: 4.5273e-05 - acc: 1.0000 - val_loss: 4.5143e-05 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      " - 0s - loss: 4.5125e-05 - acc: 1.0000 - val_loss: 4.5061e-05 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      " - 0s - loss: 4.4962e-05 - acc: 1.0000 - val_loss: 4.4824e-05 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      " - 0s - loss: 4.4807e-05 - acc: 1.0000 - val_loss: 4.4822e-05 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      " - 0s - loss: 4.4667e-05 - acc: 1.0000 - val_loss: 4.4581e-05 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      " - 0s - loss: 4.4503e-05 - acc: 1.0000 - val_loss: 4.4410e-05 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      " - 0s - loss: 4.4353e-05 - acc: 1.0000 - val_loss: 4.4263e-05 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      " - 0s - loss: 4.4202e-05 - acc: 1.0000 - val_loss: 4.4130e-05 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      " - 0s - loss: 4.4052e-05 - acc: 1.0000 - val_loss: 4.4027e-05 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      " - 0s - loss: 4.3908e-05 - acc: 1.0000 - val_loss: 4.3844e-05 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      " - 0s - loss: 4.3775e-05 - acc: 1.0000 - val_loss: 4.3678e-05 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      " - 0s - loss: 4.3614e-05 - acc: 1.0000 - val_loss: 4.3654e-05 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      " - 0s - loss: 4.3471e-05 - acc: 1.0000 - val_loss: 4.3314e-05 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      " - 0s - loss: 4.3337e-05 - acc: 1.0000 - val_loss: 4.3236e-05 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      " - 0s - loss: 4.3173e-05 - acc: 1.0000 - val_loss: 4.3108e-05 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      " - 0s - loss: 4.3025e-05 - acc: 1.0000 - val_loss: 4.2909e-05 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      " - 0s - loss: 4.2884e-05 - acc: 1.0000 - val_loss: 4.2791e-05 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      " - 0s - loss: 4.2736e-05 - acc: 1.0000 - val_loss: 4.2651e-05 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      " - 0s - loss: 4.2587e-05 - acc: 1.0000 - val_loss: 4.2523e-05 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      " - 0s - loss: 4.2448e-05 - acc: 1.0000 - val_loss: 4.2462e-05 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      " - 0s - loss: 4.2340e-05 - acc: 1.0000 - val_loss: 4.2199e-05 - val_acc: 1.0000\n",
      "Epoch 252/500\n",
      " - 0s - loss: 4.2193e-05 - acc: 1.0000 - val_loss: 4.2009e-05 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      " - 0s - loss: 4.2080e-05 - acc: 1.0000 - val_loss: 4.2042e-05 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      " - 0s - loss: 4.1890e-05 - acc: 1.0000 - val_loss: 4.1721e-05 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      " - 0s - loss: 4.1765e-05 - acc: 1.0000 - val_loss: 4.1789e-05 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      " - 0s - loss: 4.1635e-05 - acc: 1.0000 - val_loss: 4.1616e-05 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      " - 0s - loss: 4.1467e-05 - acc: 1.0000 - val_loss: 4.1311e-05 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      " - 0s - loss: 4.1363e-05 - acc: 1.0000 - val_loss: 4.1274e-05 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      " - 0s - loss: 4.1204e-05 - acc: 1.0000 - val_loss: 4.1164e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260/500\n",
      " - 0s - loss: 4.1059e-05 - acc: 1.0000 - val_loss: 4.0969e-05 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      " - 0s - loss: 4.0922e-05 - acc: 1.0000 - val_loss: 4.0845e-05 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      " - 0s - loss: 4.0785e-05 - acc: 1.0000 - val_loss: 4.0687e-05 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      " - 0s - loss: 4.0645e-05 - acc: 1.0000 - val_loss: 4.0620e-05 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      " - 0s - loss: 4.0514e-05 - acc: 1.0000 - val_loss: 4.0432e-05 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      " - 0s - loss: 4.0375e-05 - acc: 1.0000 - val_loss: 4.0344e-05 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      " - 0s - loss: 4.0243e-05 - acc: 1.0000 - val_loss: 4.0151e-05 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      " - 0s - loss: 4.0112e-05 - acc: 1.0000 - val_loss: 4.0023e-05 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      " - 0s - loss: 3.9977e-05 - acc: 1.0000 - val_loss: 3.9891e-05 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      " - 0s - loss: 3.9838e-05 - acc: 1.0000 - val_loss: 3.9727e-05 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      " - 0s - loss: 3.9720e-05 - acc: 1.0000 - val_loss: 3.9644e-05 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      " - 0s - loss: 3.9587e-05 - acc: 1.0000 - val_loss: 3.9532e-05 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      " - 0s - loss: 3.9465e-05 - acc: 1.0000 - val_loss: 3.9348e-05 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      " - 0s - loss: 3.9333e-05 - acc: 1.0000 - val_loss: 3.9330e-05 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      " - 0s - loss: 3.9220e-05 - acc: 1.0000 - val_loss: 3.9103e-05 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      " - 0s - loss: 3.9069e-05 - acc: 1.0000 - val_loss: 3.9085e-05 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      " - 0s - loss: 3.8948e-05 - acc: 1.0000 - val_loss: 3.8838e-05 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      " - 0s - loss: 3.8810e-05 - acc: 1.0000 - val_loss: 3.8783e-05 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      " - 0s - loss: 3.8686e-05 - acc: 1.0000 - val_loss: 3.8633e-05 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      " - 0s - loss: 3.8554e-05 - acc: 1.0000 - val_loss: 3.8441e-05 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      " - 0s - loss: 3.8436e-05 - acc: 1.0000 - val_loss: 3.8328e-05 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      " - 0s - loss: 3.8299e-05 - acc: 1.0000 - val_loss: 3.8291e-05 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      " - 0s - loss: 3.8185e-05 - acc: 1.0000 - val_loss: 3.8104e-05 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      " - 0s - loss: 3.8060e-05 - acc: 1.0000 - val_loss: 3.7937e-05 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      " - 0s - loss: 3.7939e-05 - acc: 1.0000 - val_loss: 3.7870e-05 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      " - 0s - loss: 3.7816e-05 - acc: 1.0000 - val_loss: 3.7726e-05 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      " - 0s - loss: 3.7689e-05 - acc: 1.0000 - val_loss: 3.7561e-05 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      " - 0s - loss: 3.7570e-05 - acc: 1.0000 - val_loss: 3.7561e-05 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      " - 0s - loss: 3.7449e-05 - acc: 1.0000 - val_loss: 3.7347e-05 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      " - 0s - loss: 3.7317e-05 - acc: 1.0000 - val_loss: 3.7287e-05 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      " - 0s - loss: 3.7202e-05 - acc: 1.0000 - val_loss: 3.7173e-05 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      " - 0s - loss: 3.7077e-05 - acc: 1.0000 - val_loss: 3.6977e-05 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      " - 0s - loss: 3.6956e-05 - acc: 1.0000 - val_loss: 3.6875e-05 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      " - 0s - loss: 3.6840e-05 - acc: 1.0000 - val_loss: 3.6772e-05 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      " - 0s - loss: 3.6716e-05 - acc: 1.0000 - val_loss: 3.6666e-05 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      " - 0s - loss: 3.6600e-05 - acc: 1.0000 - val_loss: 3.6571e-05 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      " - 0s - loss: 3.6484e-05 - acc: 1.0000 - val_loss: 3.6452e-05 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      " - 0s - loss: 3.6371e-05 - acc: 1.0000 - val_loss: 3.6286e-05 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      " - 0s - loss: 3.6259e-05 - acc: 1.0000 - val_loss: 3.6146e-05 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      " - 0s - loss: 3.6126e-05 - acc: 1.0000 - val_loss: 3.6172e-05 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      " - 0s - loss: 3.6064e-05 - acc: 1.0000 - val_loss: 3.5966e-05 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      " - 0s - loss: 3.5914e-05 - acc: 1.0000 - val_loss: 3.5770e-05 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      " - 0s - loss: 3.5791e-05 - acc: 1.0000 - val_loss: 3.5856e-05 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      " - 0s - loss: 3.5717e-05 - acc: 1.0000 - val_loss: 3.5599e-05 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      " - 0s - loss: 3.5582e-05 - acc: 1.0000 - val_loss: 3.5423e-05 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      " - 0s - loss: 3.5451e-05 - acc: 1.0000 - val_loss: 3.5546e-05 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      " - 0s - loss: 3.5380e-05 - acc: 1.0000 - val_loss: 3.5255e-05 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      " - 0s - loss: 3.5252e-05 - acc: 1.0000 - val_loss: 3.5095e-05 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      " - 0s - loss: 3.5105e-05 - acc: 1.0000 - val_loss: 3.5179e-05 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      " - 0s - loss: 3.5023e-05 - acc: 1.0000 - val_loss: 3.4927e-05 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      " - 0s - loss: 3.4894e-05 - acc: 1.0000 - val_loss: 3.4816e-05 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      " - 0s - loss: 3.4780e-05 - acc: 1.0000 - val_loss: 3.4733e-05 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      " - 0s - loss: 3.4664e-05 - acc: 1.0000 - val_loss: 3.4569e-05 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      " - 0s - loss: 3.4565e-05 - acc: 1.0000 - val_loss: 3.4498e-05 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      " - 0s - loss: 3.4445e-05 - acc: 1.0000 - val_loss: 3.4338e-05 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      " - 0s - loss: 3.4341e-05 - acc: 1.0000 - val_loss: 3.4275e-05 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      " - 0s - loss: 3.4228e-05 - acc: 1.0000 - val_loss: 3.4170e-05 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      " - 0s - loss: 3.4117e-05 - acc: 1.0000 - val_loss: 3.4091e-05 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      " - 0s - loss: 3.4011e-05 - acc: 1.0000 - val_loss: 3.3917e-05 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      " - 0s - loss: 3.3917e-05 - acc: 1.0000 - val_loss: 3.3882e-05 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      " - 0s - loss: 3.3803e-05 - acc: 1.0000 - val_loss: 3.3780e-05 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      " - 0s - loss: 3.3688e-05 - acc: 1.0000 - val_loss: 3.3583e-05 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      " - 0s - loss: 3.3596e-05 - acc: 1.0000 - val_loss: 3.3569e-05 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      " - 0s - loss: 3.3503e-05 - acc: 1.0000 - val_loss: 3.3490e-05 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      " - 0s - loss: 3.3381e-05 - acc: 1.0000 - val_loss: 3.3261e-05 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      " - 0s - loss: 3.3293e-05 - acc: 1.0000 - val_loss: 3.3237e-05 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      " - 0s - loss: 3.3178e-05 - acc: 1.0000 - val_loss: 3.3120e-05 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      " - 0s - loss: 3.3069e-05 - acc: 1.0000 - val_loss: 3.3046e-05 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      " - 0s - loss: 3.2971e-05 - acc: 1.0000 - val_loss: 3.2905e-05 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      " - 0s - loss: 3.2863e-05 - acc: 1.0000 - val_loss: 3.2863e-05 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      " - 0s - loss: 3.2758e-05 - acc: 1.0000 - val_loss: 3.2676e-05 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      " - 0s - loss: 3.2665e-05 - acc: 1.0000 - val_loss: 3.2562e-05 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      " - 0s - loss: 3.2539e-05 - acc: 1.0000 - val_loss: 3.2610e-05 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      " - 0s - loss: 3.2469e-05 - acc: 1.0000 - val_loss: 3.2397e-05 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      " - 0s - loss: 3.2352e-05 - acc: 1.0000 - val_loss: 3.2283e-05 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      " - 0s - loss: 3.2260e-05 - acc: 1.0000 - val_loss: 3.2212e-05 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      " - 0s - loss: 3.2152e-05 - acc: 1.0000 - val_loss: 3.2098e-05 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      " - 0s - loss: 3.2063e-05 - acc: 1.0000 - val_loss: 3.1952e-05 - val_acc: 1.0000\n",
      "Epoch 338/500\n",
      " - 0s - loss: 3.1944e-05 - acc: 1.0000 - val_loss: 3.1831e-05 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      " - 0s - loss: 3.1870e-05 - acc: 1.0000 - val_loss: 3.1835e-05 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      " - 0s - loss: 3.1758e-05 - acc: 1.0000 - val_loss: 3.1689e-05 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      " - 0s - loss: 3.1657e-05 - acc: 1.0000 - val_loss: 3.1610e-05 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      " - 0s - loss: 3.1563e-05 - acc: 1.0000 - val_loss: 3.1503e-05 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      " - 0s - loss: 3.1466e-05 - acc: 1.0000 - val_loss: 3.1376e-05 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      " - 0s - loss: 3.1360e-05 - acc: 1.0000 - val_loss: 3.1383e-05 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      " - 0s - loss: 3.1285e-05 - acc: 1.0000 - val_loss: 3.1226e-05 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      " - 0s - loss: 3.1197e-05 - acc: 1.0000 - val_loss: 3.1084e-05 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      " - 0s - loss: 3.1080e-05 - acc: 1.0000 - val_loss: 3.1083e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 348/500\n",
      " - 0s - loss: 3.0988e-05 - acc: 1.0000 - val_loss: 3.0899e-05 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      " - 0s - loss: 3.0896e-05 - acc: 1.0000 - val_loss: 3.0802e-05 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      " - 0s - loss: 3.0797e-05 - acc: 1.0000 - val_loss: 3.0747e-05 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      " - 0s - loss: 3.0720e-05 - acc: 1.0000 - val_loss: 3.0667e-05 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      " - 0s - loss: 3.0604e-05 - acc: 1.0000 - val_loss: 3.0511e-05 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      " - 0s - loss: 3.0520e-05 - acc: 1.0000 - val_loss: 3.0461e-05 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      " - 0s - loss: 3.0420e-05 - acc: 1.0000 - val_loss: 3.0364e-05 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      " - 0s - loss: 3.0333e-05 - acc: 1.0000 - val_loss: 3.0282e-05 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      " - 0s - loss: 3.0265e-05 - acc: 1.0000 - val_loss: 3.0147e-05 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      " - 0s - loss: 3.0154e-05 - acc: 1.0000 - val_loss: 3.0140e-05 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      " - 0s - loss: 3.0052e-05 - acc: 1.0000 - val_loss: 2.9964e-05 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      " - 0s - loss: 2.9972e-05 - acc: 1.0000 - val_loss: 2.9911e-05 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      " - 0s - loss: 2.9872e-05 - acc: 1.0000 - val_loss: 2.9877e-05 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      " - 0s - loss: 2.9782e-05 - acc: 1.0000 - val_loss: 2.9704e-05 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      " - 0s - loss: 2.9698e-05 - acc: 1.0000 - val_loss: 2.9642e-05 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      " - 0s - loss: 2.9612e-05 - acc: 1.0000 - val_loss: 2.9622e-05 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      " - 0s - loss: 2.9523e-05 - acc: 1.0000 - val_loss: 2.9433e-05 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      " - 0s - loss: 2.9435e-05 - acc: 1.0000 - val_loss: 2.9439e-05 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      " - 0s - loss: 2.9349e-05 - acc: 1.0000 - val_loss: 2.9327e-05 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      " - 0s - loss: 2.9253e-05 - acc: 1.0000 - val_loss: 2.9190e-05 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      " - 0s - loss: 2.9160e-05 - acc: 1.0000 - val_loss: 2.9133e-05 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      " - 0s - loss: 2.9073e-05 - acc: 1.0000 - val_loss: 2.8973e-05 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      " - 0s - loss: 2.8971e-05 - acc: 1.0000 - val_loss: 2.8940e-05 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      " - 0s - loss: 2.8896e-05 - acc: 1.0000 - val_loss: 2.8850e-05 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      " - 0s - loss: 2.8820e-05 - acc: 1.0000 - val_loss: 2.8781e-05 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      " - 0s - loss: 2.8746e-05 - acc: 1.0000 - val_loss: 2.8643e-05 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      " - 0s - loss: 2.8628e-05 - acc: 1.0000 - val_loss: 2.8655e-05 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      " - 0s - loss: 2.8565e-05 - acc: 1.0000 - val_loss: 2.8501e-05 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      " - 0s - loss: 2.8483e-05 - acc: 1.0000 - val_loss: 2.8409e-05 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      " - 0s - loss: 2.8389e-05 - acc: 1.0000 - val_loss: 2.8394e-05 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      " - 0s - loss: 2.8297e-05 - acc: 1.0000 - val_loss: 2.8215e-05 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      " - 0s - loss: 2.8232e-05 - acc: 1.0000 - val_loss: 2.8182e-05 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      " - 0s - loss: 2.8142e-05 - acc: 1.0000 - val_loss: 2.8208e-05 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      " - 0s - loss: 2.8076e-05 - acc: 1.0000 - val_loss: 2.8011e-05 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      " - 0s - loss: 2.7978e-05 - acc: 1.0000 - val_loss: 2.7899e-05 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      " - 0s - loss: 2.7887e-05 - acc: 1.0000 - val_loss: 2.7901e-05 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      " - 0s - loss: 2.7815e-05 - acc: 1.0000 - val_loss: 2.7766e-05 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      " - 0s - loss: 2.7722e-05 - acc: 1.0000 - val_loss: 2.7646e-05 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      " - 0s - loss: 2.7643e-05 - acc: 1.0000 - val_loss: 2.7570e-05 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      " - 0s - loss: 2.7561e-05 - acc: 1.0000 - val_loss: 2.7511e-05 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      " - 0s - loss: 2.7476e-05 - acc: 1.0000 - val_loss: 2.7464e-05 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      " - 0s - loss: 2.7394e-05 - acc: 1.0000 - val_loss: 2.7319e-05 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      " - 0s - loss: 2.7323e-05 - acc: 1.0000 - val_loss: 2.7283e-05 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      " - 0s - loss: 2.7243e-05 - acc: 1.0000 - val_loss: 2.7232e-05 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      " - 0s - loss: 2.7151e-05 - acc: 1.0000 - val_loss: 2.7076e-05 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      " - 0s - loss: 2.7090e-05 - acc: 1.0000 - val_loss: 2.7025e-05 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      " - 0s - loss: 2.6997e-05 - acc: 1.0000 - val_loss: 2.6947e-05 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      " - 0s - loss: 2.6914e-05 - acc: 1.0000 - val_loss: 2.6849e-05 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      " - 0s - loss: 2.6838e-05 - acc: 1.0000 - val_loss: 2.6760e-05 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      " - 0s - loss: 2.6751e-05 - acc: 1.0000 - val_loss: 2.6750e-05 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      " - 0s - loss: 2.6692e-05 - acc: 1.0000 - val_loss: 2.6671e-05 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      " - 0s - loss: 2.6604e-05 - acc: 1.0000 - val_loss: 2.6528e-05 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      " - 0s - loss: 2.6530e-05 - acc: 1.0000 - val_loss: 2.6476e-05 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      " - 0s - loss: 2.6451e-05 - acc: 1.0000 - val_loss: 2.6409e-05 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      " - 0s - loss: 2.6376e-05 - acc: 1.0000 - val_loss: 2.6332e-05 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      " - 0s - loss: 2.6298e-05 - acc: 1.0000 - val_loss: 2.6201e-05 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      " - 0s - loss: 2.6226e-05 - acc: 1.0000 - val_loss: 2.6194e-05 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      " - 0s - loss: 2.6151e-05 - acc: 1.0000 - val_loss: 2.6100e-05 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      " - 0s - loss: 2.6077e-05 - acc: 1.0000 - val_loss: 2.5995e-05 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      " - 0s - loss: 2.5997e-05 - acc: 1.0000 - val_loss: 2.6010e-05 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      " - 0s - loss: 2.5921e-05 - acc: 1.0000 - val_loss: 2.5858e-05 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      " - 0s - loss: 2.5844e-05 - acc: 1.0000 - val_loss: 2.5795e-05 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      " - 0s - loss: 2.5770e-05 - acc: 1.0000 - val_loss: 2.5731e-05 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      " - 0s - loss: 2.5701e-05 - acc: 1.0000 - val_loss: 2.5652e-05 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      " - 0s - loss: 2.5630e-05 - acc: 1.0000 - val_loss: 2.5598e-05 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      " - 0s - loss: 2.5543e-05 - acc: 1.0000 - val_loss: 2.5482e-05 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      " - 0s - loss: 2.5479e-05 - acc: 1.0000 - val_loss: 2.5446e-05 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      " - 0s - loss: 2.5401e-05 - acc: 1.0000 - val_loss: 2.5377e-05 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      " - 0s - loss: 2.5334e-05 - acc: 1.0000 - val_loss: 2.5285e-05 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      " - 0s - loss: 2.5267e-05 - acc: 1.0000 - val_loss: 2.5190e-05 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      " - 0s - loss: 2.5195e-05 - acc: 1.0000 - val_loss: 2.5183e-05 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      " - 0s - loss: 2.5117e-05 - acc: 1.0000 - val_loss: 2.5054e-05 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      " - 0s - loss: 2.5045e-05 - acc: 1.0000 - val_loss: 2.5016e-05 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      " - 0s - loss: 2.4963e-05 - acc: 1.0000 - val_loss: 2.4906e-05 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      " - 0s - loss: 2.4912e-05 - acc: 1.0000 - val_loss: 2.4844e-05 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      " - 0s - loss: 2.4822e-05 - acc: 1.0000 - val_loss: 2.4789e-05 - val_acc: 1.0000\n",
      "Epoch 424/500\n",
      " - 0s - loss: 2.4763e-05 - acc: 1.0000 - val_loss: 2.4703e-05 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      " - 0s - loss: 2.4687e-05 - acc: 1.0000 - val_loss: 2.4638e-05 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      " - 0s - loss: 2.4600e-05 - acc: 1.0000 - val_loss: 2.4556e-05 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      " - 0s - loss: 2.4544e-05 - acc: 1.0000 - val_loss: 2.4491e-05 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      " - 0s - loss: 2.4470e-05 - acc: 1.0000 - val_loss: 2.4429e-05 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      " - 0s - loss: 2.4403e-05 - acc: 1.0000 - val_loss: 2.4390e-05 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      " - 0s - loss: 2.4342e-05 - acc: 1.0000 - val_loss: 2.4298e-05 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      " - 0s - loss: 2.4272e-05 - acc: 1.0000 - val_loss: 2.4207e-05 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      " - 0s - loss: 2.4195e-05 - acc: 1.0000 - val_loss: 2.4202e-05 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      " - 0s - loss: 2.4141e-05 - acc: 1.0000 - val_loss: 2.4098e-05 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      " - 0s - loss: 2.4071e-05 - acc: 1.0000 - val_loss: 2.4001e-05 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      " - 0s - loss: 2.3994e-05 - acc: 1.0000 - val_loss: 2.3968e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 436/500\n",
      " - 0s - loss: 2.3935e-05 - acc: 1.0000 - val_loss: 2.3891e-05 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      " - 0s - loss: 2.3864e-05 - acc: 1.0000 - val_loss: 2.3821e-05 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      " - 0s - loss: 2.3799e-05 - acc: 1.0000 - val_loss: 2.3775e-05 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      " - 0s - loss: 2.3731e-05 - acc: 1.0000 - val_loss: 2.3659e-05 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      " - 0s - loss: 2.3641e-05 - acc: 1.0000 - val_loss: 2.3620e-05 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      " - 0s - loss: 2.3591e-05 - acc: 1.0000 - val_loss: 2.3543e-05 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      " - 0s - loss: 2.3512e-05 - acc: 1.0000 - val_loss: 2.3489e-05 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      " - 0s - loss: 2.3465e-05 - acc: 1.0000 - val_loss: 2.3425e-05 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      " - 0s - loss: 2.3395e-05 - acc: 1.0000 - val_loss: 2.3343e-05 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      " - 0s - loss: 2.3334e-05 - acc: 1.0000 - val_loss: 2.3282e-05 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      " - 0s - loss: 2.3262e-05 - acc: 1.0000 - val_loss: 2.3239e-05 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      " - 0s - loss: 2.3195e-05 - acc: 1.0000 - val_loss: 2.3153e-05 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      " - 0s - loss: 2.3132e-05 - acc: 1.0000 - val_loss: 2.3113e-05 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      " - 0s - loss: 2.3061e-05 - acc: 1.0000 - val_loss: 2.3020e-05 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      " - 0s - loss: 2.3002e-05 - acc: 1.0000 - val_loss: 2.2987e-05 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      " - 0s - loss: 2.2939e-05 - acc: 1.0000 - val_loss: 2.2918e-05 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      " - 0s - loss: 2.2874e-05 - acc: 1.0000 - val_loss: 2.2859e-05 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      " - 0s - loss: 2.2811e-05 - acc: 1.0000 - val_loss: 2.2771e-05 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      " - 0s - loss: 2.2756e-05 - acc: 1.0000 - val_loss: 2.2685e-05 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      " - 0s - loss: 2.2683e-05 - acc: 1.0000 - val_loss: 2.2653e-05 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      " - 0s - loss: 2.2632e-05 - acc: 1.0000 - val_loss: 2.2585e-05 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      " - 0s - loss: 2.2566e-05 - acc: 1.0000 - val_loss: 2.2560e-05 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      " - 0s - loss: 2.2501e-05 - acc: 1.0000 - val_loss: 2.2436e-05 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      " - 0s - loss: 2.2438e-05 - acc: 1.0000 - val_loss: 2.2403e-05 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      " - 0s - loss: 2.2390e-05 - acc: 1.0000 - val_loss: 2.2378e-05 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      " - 0s - loss: 2.2317e-05 - acc: 1.0000 - val_loss: 2.2248e-05 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      " - 0s - loss: 2.2259e-05 - acc: 1.0000 - val_loss: 2.2217e-05 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      " - 0s - loss: 2.2191e-05 - acc: 1.0000 - val_loss: 2.2154e-05 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      " - 0s - loss: 2.2126e-05 - acc: 1.0000 - val_loss: 2.2092e-05 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      " - 0s - loss: 2.2067e-05 - acc: 1.0000 - val_loss: 2.2042e-05 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      " - 0s - loss: 2.2009e-05 - acc: 1.0000 - val_loss: 2.1987e-05 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      " - 0s - loss: 2.1950e-05 - acc: 1.0000 - val_loss: 2.1916e-05 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      " - 0s - loss: 2.1888e-05 - acc: 1.0000 - val_loss: 2.1829e-05 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      " - 0s - loss: 2.1829e-05 - acc: 1.0000 - val_loss: 2.1807e-05 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      " - 0s - loss: 2.1773e-05 - acc: 1.0000 - val_loss: 2.1764e-05 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      " - 0s - loss: 2.1721e-05 - acc: 1.0000 - val_loss: 2.1655e-05 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      " - 0s - loss: 2.1650e-05 - acc: 1.0000 - val_loss: 2.1620e-05 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      " - 0s - loss: 2.1598e-05 - acc: 1.0000 - val_loss: 2.1575e-05 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      " - 0s - loss: 2.1548e-05 - acc: 1.0000 - val_loss: 2.1484e-05 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      " - 0s - loss: 2.1481e-05 - acc: 1.0000 - val_loss: 2.1450e-05 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      " - 0s - loss: 2.1419e-05 - acc: 1.0000 - val_loss: 2.1381e-05 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      " - 0s - loss: 2.1362e-05 - acc: 1.0000 - val_loss: 2.1341e-05 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      " - 0s - loss: 2.1311e-05 - acc: 1.0000 - val_loss: 2.1254e-05 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      " - 0s - loss: 2.1238e-05 - acc: 1.0000 - val_loss: 2.1213e-05 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      " - 0s - loss: 2.1191e-05 - acc: 1.0000 - val_loss: 2.1123e-05 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      " - 0s - loss: 2.1125e-05 - acc: 1.0000 - val_loss: 2.1110e-05 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      " - 0s - loss: 2.1073e-05 - acc: 1.0000 - val_loss: 2.1025e-05 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      " - 0s - loss: 2.1018e-05 - acc: 1.0000 - val_loss: 2.0988e-05 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      " - 0s - loss: 2.0959e-05 - acc: 1.0000 - val_loss: 2.0927e-05 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      " - 0s - loss: 2.0904e-05 - acc: 1.0000 - val_loss: 2.0875e-05 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      " - 0s - loss: 2.0851e-05 - acc: 1.0000 - val_loss: 2.0830e-05 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      " - 0s - loss: 2.0793e-05 - acc: 1.0000 - val_loss: 2.0760e-05 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      " - 0s - loss: 2.0750e-05 - acc: 1.0000 - val_loss: 2.0692e-05 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      " - 0s - loss: 2.0669e-05 - acc: 1.0000 - val_loss: 2.0661e-05 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      " - 0s - loss: 2.0624e-05 - acc: 1.0000 - val_loss: 2.0608e-05 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      " - 0s - loss: 2.0573e-05 - acc: 1.0000 - val_loss: 2.0551e-05 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      " - 0s - loss: 2.0522e-05 - acc: 1.0000 - val_loss: 2.0470e-05 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      " - 0s - loss: 2.0465e-05 - acc: 1.0000 - val_loss: 2.0408e-05 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      " - 0s - loss: 2.0403e-05 - acc: 1.0000 - val_loss: 2.0356e-05 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      " - 0s - loss: 2.0350e-05 - acc: 1.0000 - val_loss: 2.0316e-05 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      " - 0s - loss: 2.0292e-05 - acc: 1.0000 - val_loss: 2.0277e-05 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      " - 0s - loss: 2.0248e-05 - acc: 1.0000 - val_loss: 2.0182e-05 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      " - 0s - loss: 2.0173e-05 - acc: 1.0000 - val_loss: 2.0150e-05 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      " - 0s - loss: 2.0129e-05 - acc: 1.0000 - val_loss: 2.0104e-05 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      " - 0s - loss: 2.0075e-05 - acc: 1.0000 - val_loss: 2.0065e-05 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#batch_size = 450\n",
    "# tune more hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "verb = 2\n",
    "model = keras_nn_classifier(learning_rate)\n",
    "# convert data into numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "history = model.fit(x_train, y_train, batch_size = batch_size, \n",
    "                   epochs = epochs, validation_data = (x_val, y_val),\n",
    "                   verbose = verb, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4885c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a49d871d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Accuracy:  1.0\n",
      "Average Validation Accuracy:  1.0\n",
      "Last Training Accuracy:  1.0\n",
      "Last Validation Accuracy:  1.0\n",
      "Average of last 10 validation Accuracies:  1.0\n"
     ]
    }
   ],
   "source": [
    "sum = 0.0\n",
    "for val in history.history['acc']:\n",
    "    sum+=val\n",
    "avg = sum/epochs\n",
    "print(\"Average Training Accuracy: \", avg)\n",
    "\n",
    "\n",
    "sum = 0.0\n",
    "for val in history.history['val_acc']:\n",
    "    sum+=val\n",
    "avg = sum/epochs\n",
    "print(\"Average Validation Accuracy: \", avg)\n",
    "\n",
    "print(\"Last Training Accuracy: \", history.history['acc'][epochs-1])\n",
    "print(\"Last Validation Accuracy: \", history.history['val_acc'][epochs-1])\n",
    "\n",
    "\n",
    "sum = 0.0\n",
    "for i in range (epochs-10, epochs):\n",
    "    sum+= history.history['val_acc'][i]\n",
    "avg = sum / 10.0\n",
    "print(\"Average of last 10 validation Accuracies: \", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_jobs = 10, n_estimators = 100, max_features = num_features, max_depth = 15, \n",
    "                             min_samples_split = 100, min_samples_leaf = 30, min_weight_fraction_leaf = 0., \n",
    "                            max_leaf_nodes = 50, min_impurity_decrease = 0.001, bootstrap = True, \n",
    "                            oob_score = False, verbose = 0, criterion = \"gini\")\n",
    "clf.fit(x_train, y_train)\n",
    "predicted = clf.predict(x_val)\n",
    "correct = 0\n",
    "for i in range(0, len(predicted)):\n",
    "    if (y_val[i][0] == predicted[i][0]):\n",
    "        if (y_val[i][1] == predicted[i][1]):\n",
    "            #if (y_val[i][2] == predicted[i][2]):\n",
    "                correct+=1\n",
    "print(correct / len(predicted))\n",
    "print(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
